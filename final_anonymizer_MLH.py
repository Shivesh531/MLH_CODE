# -*- coding: utf-8 -*-
"""final_anonymizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eTXgWVMEVDMlmNFdW7JLzchIESrbNFfw
"""

#@title Patient data(INPUT)
name='John Doe ' #@param {type:'string'}
doctor='Dr. Mark Anderson ' #@param {type:'string'}
adm_no='ISP-2543-3214564590' #@param {type:'string'}
email='jondoe1936@gmail.com' #@param {type:'string'}
phone='9831-215855' #@param {type:'string'}
date = "2023-07-07" #@param {type:"date"}
diagnosis='1.\tPrimary Diagnosis: Incisional Hernia 2.\tSecondary Diagnosis: None' #@param {type:'string'}
course_in_hospital= 'During the hospital stay, Mr. Doe underwent the following treatments and interventions: \u2022\tOpen surgical repair of the incisional hernia under general anesthesia. \u2022\tReduction of hernia sac and placement of mesh for reinforcement of the abdominal wall. \u2022\tPostoperative pain management with analgesics and close monitoring of vital signs. \u2022\tRoutine wound care and monitoring for signs of infection.'   #@param {type:'string'}
medications= 'Upon discharge, Mr. Doe has been prescribed the following medications: 1.\tAcetaminophen 500mg: Take 2 tablets every 6 hours as needed for pain. 2.\tIbuprofen 400mg: Take 1 tablet every 8 hours as needed for pain and inflammation. 3.\tAntibiotic (Amoxicillin 500mg): Take 1 tablet three times daily for 7 days.'   #@param {type:'string'}
follow_up= '\u2022\tMr. Doe is advised to schedule a follow-up appointment with Dr. Emily Johnson, his primary care physician, within two weeks for a wound evaluation and further assessment. \u2022\tIf any signs of infection, persistent pain, or recurrence of the hernia develop, he should seek immediate medical attention.'   #@param {type:'string'}
activity_dietary= "\u2022\tMr. Doe should avoid heavy lifting (over 10 pounds) and strenuous activities for the next 4 to 6 weeks. \u2022\tGradual increase in activity is encouraged, while listening to the body's response. \u2022\tRegular daily activities, such as walking and light household chores, are permitted."   #@param {type:'string'}
discharge_instruction= '\u2022\tMr. Doe should continue taking prescribed medications as instructed. \u2022\tHe should follow a balanced diet, rich in fiber and protein, to aid in wound healing. \u2022\tIt is important to keep the surgical incision clean and dry, and report any signs of infection (redness, swelling, discharge) to the primary care physician. \u2022\tMr. Doe should attend all scheduled follow-up appointments and tests as recommended.'   #@param {type:'string'}

text= name+' '+doctor+' '+adm_no+' '+date+' '+email+' '+phone+' '+course_in_hospital+' '+medications+' '+follow_up+' '+activity_dietary+' '+discharge_instruction+' '

!pip install transformers

import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers.pipelines.token_classification import TokenClassificationPipeline
import spacy
from spacy.matcher import Matcher
from spacy.lang.en import English
from spacy.matcher import PhraseMatcher

#WRITE YOUR INPUT HERE
text="Dr vijaya Kumar works at FORTIS HOSPITALS in Kolkta and his email is vijay_fortis546@gmail.com, number is (+91) 9831-716875 working on patient with admission number ISP-1233-1234567890 and DOB  2-6-1969"

model_checkpoint = "Davlan/bert-base-multilingual-cased-ner-hrl"

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)


class TokenClassificationChunkPipeline(TokenClassificationPipeline):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def preprocess(self, sentence, offset_mapping=None, **preprocess_params):
        tokenizer_params = preprocess_params.pop("tokenizer_params", {})
        truncation = True if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else False
        inputs = self.tokenizer(
            sentence,
            return_tensors="pt",
            truncation=True,
            return_special_tokens_mask=True,
            return_offsets_mapping=True,
            return_overflowing_tokens=True,  # Return multiple chunks
            max_length=self.tokenizer.model_max_length,
            padding=True
        )
        #inputs.pop("overflow_to_sample_mapping", None)
        num_chunks = len(inputs["input_ids"])

        for i in range(num_chunks):
            if self.framework == "tf":
                model_inputs = {k: tf.expand_dims(v[i], 0) for k, v in inputs.items()}
            else:
                model_inputs = {k: v[i].unsqueeze(0) for k, v in inputs.items()}
            if offset_mapping is not None:
                model_inputs["offset_mapping"] = offset_mapping
            model_inputs["sentence"] = sentence if i == 0 else None
            model_inputs["is_last"] = i == num_chunks - 1
            yield model_inputs

    def _forward(self, model_inputs):
        # Forward
        special_tokens_mask = model_inputs.pop("special_tokens_mask")
        offset_mapping = model_inputs.pop("offset_mapping", None)
        sentence = model_inputs.pop("sentence")
        is_last = model_inputs.pop("is_last")

        overflow_to_sample_mapping = model_inputs.pop("overflow_to_sample_mapping")

        output = self.model(**model_inputs)
        logits = output["logits"] if isinstance(output, dict) else output[0]


        model_outputs = {
            "logits": logits,
            "special_tokens_mask": special_tokens_mask,
            "offset_mapping": offset_mapping,
            "sentence": sentence,
            "overflow_to_sample_mapping": overflow_to_sample_mapping,
            "is_last": is_last,
            **model_inputs,
        }

        # We reshape outputs to fit with the postprocess inputs
        model_outputs["input_ids"] = torch.reshape(model_outputs["input_ids"], (1, -1))
        model_outputs["token_type_ids"] = torch.reshape(model_outputs["token_type_ids"], (1, -1))
        model_outputs["attention_mask"] = torch.reshape(model_outputs["attention_mask"], (1, -1))
        model_outputs["special_tokens_mask"] = torch.reshape(model_outputs["special_tokens_mask"], (1, -1))
        model_outputs["offset_mapping"] = torch.reshape(model_outputs["offset_mapping"], (1, -1, 2))

        return model_outputs



pipe = TokenClassificationChunkPipeline(model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# Replace entities
def anonymize(text):
    ents = pipe(text)
    split_text = list(text)
    for ent in ents:
        #split_text[ent['start']] = f"[{ent['entity_group']}]"
        split_text[ent['start']] = f"*****"
        for i in range(ent['start'] + 1, ent['end']):
            split_text[i] = ""

    return "".join(split_text)



text = anonymize(text)

nlp = spacy.load("en_core_web_sm")
matcher1 = Matcher(nlp.vocab)
pattern = [{"TEXT":{"REGEX":"[a-zA-Z0-9_.]+@[a-zA-Z0-9_.]+"}}]
matcher1.add("EMAIL",[pattern])

nlp = English()
matcher = PhraseMatcher(nlp.vocab, attr="SHAPE")
matcher.add("ADMISSION_NUMBER", [nlp("ISP-0000-0000000000")])

matcher.add("DATE", [nlp("00-00-0000"),nlp("0-00-0000"),nlp("00-0-0000"),nlp("0-0-0000"),nlp("00-00-00"),nlp("0-00-00"),nlp("00-0-00"),nlp("0-0-00"),nlp("0000-00-00")])

matcher.add("PHONE_NUMBER", [nlp("(+91) 0000-000000"),nlp("(91) 0000-000000"),nlp("0000-000000")])

doc= nlp(text)
matches= matcher(doc)
anonymized=""

lst =[t.text for t in doc]
for i in range(len(matches)):
  for j in range(matches[i][1],matches[i][2]):
    lst[j]="**"
matches= matcher1(doc)
for i in range(len(matches)):
  for j in range(matches[i][1],matches[i][2]):
    lst[j]="**"
for ele in lst:
  anonymized+=ele+" "

print(anonymized)

!pip freeze > requirements.txt